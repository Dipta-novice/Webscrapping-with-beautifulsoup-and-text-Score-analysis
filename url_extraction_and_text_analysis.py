# -*- coding: utf-8 -*-
"""Url extraction and text analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H5_mN3tqWOVGwFDwIIRhVZ8GiyiOZMzb
"""

import pandas as pd
import requests
from nltk.corpus import stopwords 
from nltk.tokenize import word_tokenize
from nltk.tokenize import sent_tokenize
from bs4 import BeautifulSoup
import re
import nltk

from nltk.sentiment.vader import SentimentIntensityAnalyzer
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
# In[2]:


nltk.download('punkt')
nltk.download("stopwords")

import nltk
nltk.download('vader_lexicon')

from google.colab import files
uploaded=files.upload()

df = pd.read_excel('Input.xlsx')

df.head(5)

links=[x for x in df['URL']]

links[0]

from google.colab import files
uploaded=files.upload()

op=pd.read_excel('Output Data Structure.xlsx')

op

headers = {
    "User-Agent": "Mozilla/5.0 (X11; Linux x86_64; rv:60.0) Gecko/20100101 Firefox/60.0"
}

page = requests.get(links[0], headers=headers)

soup = BeautifulSoup(page.content, 'html.parser')

soup

all=soup.find("div",{"class":"td-post-content"})

# all

import string

reports=[]
for i in links:
    page = requests.get(i, headers=headers)
    soup = BeautifulSoup(page.content,'html.parser')
    e=soup.find("div",{"class":"td-post-content"})
    reports.append(e.get_text())

reports[150]

average_sentence_lengths=[]

for i in range(len(reports)):
  text=reports[i]
  sentences=text.split(".")
  total_length=0
  for j in sentences:
    length=len(j.split())
    total_length += length
  average_sentence_lengths.append(total_length/(len(sentences)+0.00001))

print(average_)



tokenized=[]
for i in reports:
  tokenized.append(word_tokenize(i, "english"))

print(stopwords.words('english'))

type(final_words)

dirt=[',','.',' ','(',')',]
clean=[]
for i in final_words:
  cln=[]
  for j in i:
    if j not in dirt:
      cln.append(j.upper())
  clean.append(cln)

# clean[0]

from google.colab import files
uploaded=files.upload()

dicti=pd.read_csv('LoughranMcDonald_MasterDictionary_2020.csv')

positive_dictionary = [x for x in dicti[dicti['Positive'] != 0]['Word']]

negative_dictionary = [x for x in dicti[dicti['Negative'] != 0]['Word']]

print(f"Total positve words in dictionary are {len(positive_dictionary)}")
print(f"Total negative words in dictionary are {len(negative_dictionary)}")

len(positive_dictionary)



def syllable_morethan2(word):
    if(len(word) > 2 and (word[-2:] == 'es' or word[-2:] == 'ed')):
        return 0
    
    count =0
    vowels = ['a','e','i','o','u']
    for i in word:
        if(i.lower() in vowels):
            count = count +1
        
    if(count > 2):
        return 1
    else:
        return 0

pp=['I','we','my','ours','us','your','you','his','her','me','he','she','him']

final_list=[]
POSITIVE_SCORES=[]
NEGETIVE_SCORES=[]
polarity=[]
subjectivity=[]
count_of_complex_words=[]
fog_index_cal=[]
list_of_avg_word_length=[]
word_counts=[]
personal_pronouns=[]


# for k  in range(len(clean)):
#   i=clean[k]

for i in clean:
  pw=[]
  nw=[]
  kk=[]
  p=0
  n=0
  k=0
  count_of_complex_word = 0
  total_word_length = 0
  pronouns = 0

  for j in i:
    total_word_length += len(j)
    count_of_complex_word+=syllable_morethan2(j)

    if j in pp:
      pronouns +=1

    if j in positive_dictionary:
      pw.append(j)
      p+=1
    elif j in negative_dictionary:
      nw.append(j)
      n+=1
    else:
      k+=1

  total_words=p+n+k
  percentage_complexwords=count_of_complex_word/(total_words+0.00001)
  avg_word_length=total_word_length/(total_words+0.00001)
  final_list.append([pw,nw,kk])

  
  POSITIVE_SCORES.append(p)
  NEGETIVE_SCORES.append(n)
  polarity.append((p-n)/(p+n+0.000001))
  subjectivity.append((p+n)/(total_words+0.00001))
  count_of_complex_words.append(count_of_complex_word)
  fog_index_cal.append(0.4*(average_sentence_lengths[clean.index(i)]/percentage_complexwords))
  word_counts.append(total_words)
  personal_pronouns.append(pronouns)
  count_of_complex_words.append(count_of_complex_word)

pcx=[]
for i in range(0,169):
  x=(count_of_complex_words[i]/word_counts[i])
  pcx.append(x)

anws=[]
for i in range(0,169):
  x=op['WORD COUNT'][i]/op['AVG SENTENCE LENGTH'][i]
  anws.append(x)

for i in range(0,169):
  op['POSITIVE SCORE'][i]=POSITIVE_SCORES[i]
  op['NEGATIVE SCORE'][i]=NEGETIVE_SCORES[i]
  op['POLARITY SCORE'][i]=polarity[i]
  op['SUBJECTIVITY SCORE'][i]=subjectivity[i]
  op['AVG SENTENCE LENGTH'][i]=average_sentence_lengths[i]
  op['PERCENTAGE OF COMPLEX WORDS'][i]=pcx[i]
  op['FOG INDEX'][i]=fog_index_cal[i]
  op['COMPLEX WORD COUNT'][i]=count_of_complex_words[i]
  op['WORD COUNT'][i]=word_counts[i]
  op['PERSONAL PRONOUNS'][i]=personal_pronouns[i]
  op['AVG NUMBER OF WORDS PER SENTENCE'][i]=anws[i]
  op['AVG WORD LENGTH'][i]=awl[i]

op

